{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64048f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing relevant libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBT Account Id and API Key\n",
    "account_id= \"your_account_id\"\n",
    "API_TOKEN = \"your_api_key\" \n",
    "\n",
    "# Database connection details\n",
    "username = \"your_username\"\n",
    "password = \"your_password\"\n",
    "host = \"your_host\"  # Or your database host\n",
    "port = \"your_port\"  # Default PostgreSQL port\n",
    "database = \"your_database\"\n",
    "schema=\"your_schema\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fafde",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = f'https://cloud.getdbt.com/api/v2/accounts/{account_id}/jobs/'\n",
    "\n",
    "# Headers for authentication\n",
    "headers = {\n",
    "    \"Authorization\": f\"Token {API_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "total_jobs = requests.get(BASE_URL, headers=headers).json().get(\"extra\")['pagination']['total_count']\n",
    "\n",
    "print(\"Total jobs for this dbt account are \", total_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pagination parameters\n",
    "limit = 100  # Maximum number of records per request\n",
    "total_records = total_jobs  # Total number of records\n",
    "records_to_fetch = total_jobs\n",
    "\n",
    "# Calculate the starting offset\n",
    "start_offset = total_records - records_to_fetch\n",
    "\n",
    "# Initialize an empty list to store the records\n",
    "last_records = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcde9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Loop through and fetch 100 records at a time until we have all the records\n",
    "offset = start_offset\n",
    "while offset < total_records:\n",
    "    # Make the API request with limit and offset\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "    response = requests.get(BASE_URL, headers=headers, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        runs = data.get(\"data\", [])\n",
    "        last_records.extend(runs)\n",
    "\n",
    "        # Increment offset by the limit\n",
    "        offset += limit\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}, {response.text}\")\n",
    "        break\n",
    "\n",
    "# Convert the results to a pandas DataFrame\n",
    "df = pd.DataFrame(last_records)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c5fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768da263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all timestamp columns to datetime\n",
    "dt_columns= ['created_at','updated_at','next_run']\n",
    "for column in dt_columns :\n",
    "    try:\n",
    "        df[column] = pd.to_datetime(df[column])\n",
    "    except Exception:\n",
    "        pass  # Skip non-datetime columns\n",
    "\n",
    "# Define the timezone conversion\n",
    "utc = pytz.utc\n",
    "ist = pytz.timezone(\"Asia/Kolkata\")\n",
    "\n",
    "# Convert all datetime columns from UTC to IST\n",
    "for column in df.select_dtypes(include=['datetime64[ns]']).columns:\n",
    "    df[column] = df[column].dt.tz_localize(utc).dt.tz_convert(ist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a79769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import json_normalize\n",
    "\n",
    "# Flatten dict columns and merge with original DataFrame\n",
    "for column in df.columns:\n",
    "    if df[column].apply(lambda x: isinstance(x, dict)).any():\n",
    "        flat_df = json_normalize(df[column])\n",
    "        flat_df.columns = [f\"{column}_{subcol}\" for subcol in flat_df.columns]\n",
    "        df = df.drop(columns=[column]).join(flat_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ace4c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0645c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab90054",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('dbt_jobs',con=engine, schema=schema, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
